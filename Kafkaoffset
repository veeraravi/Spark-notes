import org.apache.kafka.clients.consumer.{KafkaConsumer, OffsetAndTimestamp}
import org.apache.kafka.common.TopicPartition
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.kafka010._
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent

import java.util.Properties
import java.time.Instant
import scala.collection.JavaConverters._

object KafkaOffsetOrTimestampReader {
  def main(args: Array[String]): Unit = {
    if (args.length < 2) {
      println("Usage: KafkaOffsetOrTimestampReader <kafkaBootstrapServers> <kafkaTopic> [<startTimestamp> | <startOffsets>]")
      System.exit(1)
    }

    val kafkaBootstrapServers = args(0)
    val kafkaTopic = args(1)
    val startTimestampOrOffsets = args(2)

    val conf = new SparkConf().setAppName("KafkaOffsetOrTimestampReader").setMaster("local[*]")
    val sc = new SparkContext(conf)

    // Kafka consumer properties
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> kafkaBootstrapServers,
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "spark-kafka-consumer-group"
    )

    // Determine the starting offsets
    val fromOffsets = if (startTimestampOrOffsets.matches("\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3}Z")) {
      // Handle as timestamp
      val startTimeMillis = Instant.parse(startTimestampOrOffsets).toEpochMilli
      getOffsetsForTimestamp(kafkaParams, kafkaTopic, startTimeMillis)
    } else {
      // Handle as offsets
      parseOffsets(startTimestampOrOffsets)
    }

    // Create RDD from Kafka using the obtained offsets
    val kafkaRDD = KafkaUtils.createRDD[String, String](
      sc,
      kafkaParams,
      fromOffsets,
      PreferConsistent
    )

    // Process the data
    kafkaRDD.foreach { record =>
      val key = record.key()
      val value = record.value()
      println(s"Key: $key, Value: $value")
    }

    sc.stop()
  }

  def getOffsetsForTimestamp(kafkaParams: Map[String, Object], kafkaTopic: String, timestamp: Long): Map[TopicPartition, Long] = {
    val props = new Properties()
    kafkaParams.foreach { case (k, v) => props.put(k, v) }
    val consumer = new KafkaConsumer[String, String](props)

    // Get the partitions for the topic
    val partitions = consumer.partitionsFor(kafkaTopic).asScala.map { info =>
      new TopicPartition(info.topic(), info.partition())
    }

    // Fetch the offsets for the specific timestamp
    val timestampsToSearch = partitions.map(tp => (tp, timestamp)).toMap.asJava
    val offsetsForTimes = consumer.offsetsForTimes(timestampsToSearch)

    // Create a map of TopicPartition to offset
    val fromOffsets = offsetsForTimes.asScala.collect {
      case (tp, offsetAndTimestamp: OffsetAndTimestamp) if offsetAndTimestamp != null =>
        tp -> offsetAndTimestamp.offset()
    }.toMap

    // Close the consumer
    consumer.close()
    fromOffsets
  }

  def parseOffsets(offsetsStr: String): Map[TopicPartition, Long] = {
    offsetsStr.split(",").map { offset =>
      val parts = offset.split(":")
      new TopicPartition(parts(0), parts(1).toInt) -> parts(2).toLong
    }.toMap
  }
}










